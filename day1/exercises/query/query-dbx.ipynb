{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86969cb6-e436-4eee-a751-4b7f79b70460",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load data (public)\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"wasbs://sampledata@azuresynapsestorage.blob.core.windows.net/WideWorldImportersDW/csv/full/dimension_customer\")\n",
    "\n",
    "# Managed Delta table using saveAsTable()\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dimCustomer_manag1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4411de91-a445-43d7-9145-838b4994293d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write to json, parquet, csv and delta\n",
    "df.write.format(\"json\").mode(\"overwrite\").save(\"abfss://default@<>.dfs.core.windows.net/d1/json/\")\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"abfss://default@<>.dfs.core.windows.net/d1/csv/\")\n",
    "df.write.format(\"parquet\").mode(\"overwrite\").save(\"abfss://default@<>.dfs.core.windows.net/d1/parquet/\")\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"abfss://default@<>.dfs.core.windows.net/d1/delta/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "678aa793-d8a7-40af-984e-7a5ca6c0c21a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load json from adls gen2\n",
    "df_json = spark.read.format(\"json\").load(\"abfss://default@<>.dfs.core.windows.net/d1/json\")\n",
    "df_json.show(10)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2023-10-17 11:47:55",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
